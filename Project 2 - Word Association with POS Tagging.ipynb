{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8eb03b",
   "metadata": {},
   "source": [
    "# Preprocessing and Word Association with POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c42ab5",
   "metadata": {},
   "source": [
    "On this notebook is developed the preprocessing and word association with tagging to our corpus made by newspaper texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a6aef",
   "metadata": {},
   "source": [
    "## Preprocessing: Normalization with POS tagging\n",
    "\n",
    "First, we are going to define our functions to prepare our data by tagging each word with its respective Part of Speech. This tagging is done to get better results when we were looking for word associations, because a priori those tags will help us to filter the words that belong to the same POS and then apply the similarity measures detailed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debe9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc791b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_corpus(path):\n",
    "    # Getting corpus from the directory\n",
    "    corpus = nltk.corpus.PlaintextCorpusReader(path, '.*')\n",
    "    file_list = corpus.fileids()\n",
    "    \n",
    "    # Reuniting all text content from files on the directory\n",
    "    all_text = ''\n",
    "    for file in file_list:\n",
    "        with open(path + file, encoding = 'utf-8') as rfile:\n",
    "            text = rfile.read()\n",
    "            all_text += text\n",
    "    \n",
    "    # Cleaning HTML tags\n",
    "    soup = BeautifulSoup(all_text, 'lxml')\n",
    "    clean_text = soup.get_text()\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa2c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_words(text):\n",
    "    words = text.split()\n",
    "    alphabetic_words = list()\n",
    "    \n",
    "    for word in words:\n",
    "        token = []\n",
    "        for character in word:\n",
    "            if re.match(r'^[a-záéíóúñü+$]', character):\n",
    "                token.append(character)\n",
    "        token = ''.join(token)\n",
    "        if token != '':\n",
    "            alphabetic_words.append(token)\n",
    "    \n",
    "    return alphabetic_words\n",
    "\n",
    "def tokenize_by_sents(text):\n",
    "    tokens = nltk.data.load(\"tokenizers/punkt/spanish.pickle\") \n",
    "    sents = tokens.tokenize(text)\n",
    "    alphabetic_sents = list()\n",
    "    \n",
    "    for sent in sents:\n",
    "        sent_token = tokenize_by_words(sent)\n",
    "        alphabetic_sents.append(sent_token)\n",
    "    \n",
    "    return alphabetic_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fd2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_from_sents(sents, path = './stopwords_es.txt'):\n",
    "    with open(path, encoding = 'utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]\n",
    "        \n",
    "    clean_sents = list()\n",
    "    for sent in sents:\n",
    "        clean_sent = [word for word in sent if word not in stop_words]\n",
    "        clean_sents.append(clean_sent)\n",
    "    \n",
    "    return clean_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1cb9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "from pickle import dump\n",
    "\n",
    "def make_and_save_spanish_tagger(fname):\n",
    "        \n",
    "    tags_sents = list()\n",
    "    for sent in cess_esp.tagged_sents():\n",
    "        tags_sents_aux = [tag for (word, tag) in sent]\n",
    "        tags_sents = tags_sents + tags_sents_aux\n",
    "    \n",
    "    most_used_tag_sents = nltk.FreqDist(tags_sents).max()\n",
    "    \n",
    "    default_tagger = nltk.DefaultTagger(most_used_tag_sents)\n",
    "    \n",
    "    patterns = [\n",
    "        (r'.*o$', 'n'),\n",
    "        (r'.*os$', 'n'),\n",
    "        (r'.*a$', 'n'),\n",
    "        (r'.*as$', 'n'),\n",
    "        (r'.*e$', 'n'),\n",
    "        (r'.*es$', 'n'),\n",
    "        (r'.^[0-9]+$', 'z')\n",
    "    ]\n",
    "    \n",
    "    regexp_tagger = nltk.RegexpTagger(patterns, backoff = default_tagger)\n",
    "    \n",
    "    cess_tagged_sents = cess_esp.tagged_sents()\n",
    "    spanish_tagger = nltk.UnigramTagger(cess_tagged_sents, backoff = regexp_tagger)\n",
    "    \n",
    "    output = open(fname, 'wb')\n",
    "    dump(spanish_tagger, output, -1)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b0d3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_and_save_spanish_tagger('./spanish_tagger.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55cb5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "def tag(text, path = './spanish_tagger.pkl'):\n",
    "    input_f = open(path, 'rb')\n",
    "    tagger = load(input_f)\n",
    "    input_f.close()\n",
    "    tagged_sentences = [tagger.tag(sent) for sent in text]\n",
    "    \n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b9f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_from_sents(text, path = './generate.txt'):\n",
    "    \n",
    "    lemmas = dict()\n",
    "    with open(path, encoding = 'latin1') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                tag = words[-2].strip()\n",
    "                tag = tag[0].lower()\n",
    "                lemmas[(token, tag)] = (lemma, tag)\n",
    "    \n",
    "    lemmatized_text = list()\n",
    "    for sent in text:\n",
    "        #words = sent.split()\n",
    "        lemmatized_sent = list()\n",
    "        for word in sent:\n",
    "            if word in lemmas.keys():\n",
    "                lemmatized_sent.append(lemmas[word])\n",
    "            else:\n",
    "                lemmatized_sent.append(word)\n",
    "        \n",
    "        lemmatized_text.append(lemmatized_sent)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9835be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_by_sents(path):\n",
    "    clean_text = extract_text_from_corpus(path)\n",
    "    alphabetic_sents = tokenize_by_sents(clean_text)     \n",
    "    clean_sents = remove_stop_words_from_sents(alphabetic_sents)\n",
    "    tagged_sents = tag(text = clean_sents)\n",
    "    \n",
    "    new_tagged_sents = list()\n",
    "    for sent in tagged_sents:\n",
    "        new_sent = list()\n",
    "        for element in sent:\n",
    "            word = element[0]\n",
    "            tag_elem = element[1]\n",
    "            new_sent.append((word, tag_elem[0].lower()))\n",
    "        new_tagged_sents.append(new_sent)\n",
    "    \n",
    "    \n",
    "    preprocessed_text_sents = lemmatize_from_sents(text = new_tagged_sents)\n",
    "    #print('\\n\\033[1mLemmatization from sentence completed\\033[0m')\n",
    "    #print(f'Some words after lemmatization: \\n{preprocessed_text_sents[:200]}')\n",
    "    \n",
    "    return preprocessed_text_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601e89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path = './../EXCELSIOR_100_files/'):\n",
    "    try:\n",
    "        preprocessed_text_sents = normalize_by_sents(path)\n",
    "        print('\\033[1mNormalization by sentence tokens completed\\033[0m')\n",
    "        #print(f'Some words after normalization: \\n{clean_sents[:200]}')\n",
    "\n",
    "        return preprocessed_text_sents\n",
    "\n",
    "    except Exception as e:\n",
    "        print('An error has occured: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc78f24c",
   "metadata": {},
   "source": [
    "#### Preprocessing with sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb13d17d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNormalization by sentence tokens completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text_sents = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "064e257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list()\n",
    "for sent in preprocessed_text_sents:\n",
    "    for word in sent:\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e42fbfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6292"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(sorted(set(words)))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c0cda7",
   "metadata": {},
   "source": [
    "## Word associations\n",
    "\n",
    "Once we have completed our preprocessing, it's time to define the functions to find the similarity between words, hence get the word associations in our corpus. In order to do that, we must have functions to extract the contexts from the preprocessed text with sentence tokens. Then, we also need a function to calculate the probability's vector for each word. And finally, functions to quantify the similarity between words based on the dot product or the cosine measure between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75f60fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts_sents(vocabulary, text, window = 8):\n",
    "    contexts = dict()\n",
    "    for w in vocabulary:\n",
    "        context = list()\n",
    "        for sent in text:\n",
    "            for i in range(len(sent)):\n",
    "                if sent[i] == w:\n",
    "                    for j in range(i - int(window / 2), i):\n",
    "                        if j >= 0:\n",
    "                            context.append(sent[j])\n",
    "                    try:\n",
    "                        for j in range(i + 1, i + (int(window / 2) + 1)):\n",
    "                            context.append(sent[j])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "        contexts[w] = context\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b73d5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_vectors(vocabulary, contexts):\n",
    "    probs = dict()\n",
    "    for v in vocabulary:\n",
    "        context = contexts[v]\n",
    "        vector_normalized = []\n",
    "        for voc in vocabulary:\n",
    "            vector_normalized.append(context.count(voc))\n",
    "        vector_normalized = np.array(vector_normalized)\n",
    "        s = np.sum(vector_normalized)\n",
    "        if s != 0:\n",
    "            vector_normalized = vector_normalized / s\n",
    "        probs[v] = vector_normalized\n",
    "    return probs\n",
    "\n",
    "\n",
    "def s_dot_product(word, vectors, aux_path = ''):\n",
    "\n",
    "    similarities = dict()\n",
    "    v = vectors[word]\n",
    "    for w in vectors.keys():\n",
    "        similarities[w] = np.dot(vectors[w], v)\n",
    "    similarities = (sorted(similarities.items(), key = lambda item: item[1], reverse = True))\n",
    "    \n",
    "    with open('./dot_product_tag/similar_words_to_' + word[0] + '_' + word[1] + '_with_dot_product_of_' + aux_path + '.txt', 'w', encoding = 'utf-8') as f:\n",
    "        for item in similarities:\n",
    "            f.write(str(item) + '\\n')\n",
    "\n",
    "\n",
    "def s_cosine(word, vectors, aux_path = ''):\n",
    "    similarities = dict()\n",
    "    v = vectors[word]\n",
    "    for w in vectors.keys():\n",
    "        v2 = vectors[w]\n",
    "        norm1 = np.linalg.norm(v)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            similarities[w] = 0\n",
    "        else:\n",
    "            similarities[w] = np.dot(v, v2) / (norm1 * norm2)\n",
    "    similarities = (sorted(similarities.items(), key = lambda item: item[1], reverse = True))\n",
    "    \n",
    "    with open('./cosine_tag/similar_words_to_' + word[0] + '_' + word[1] + '_with_cosine_of_' + aux_path + '.txt', 'w', encoding = 'utf-8') as f:\n",
    "        for item in similarities:\n",
    "            f.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "795085e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(word, vectors, aux_path = '', tf_idf = False, dot_product = False, cosine = False):   \n",
    "    tag = word[1]\n",
    "    new_vectors = dict()\n",
    "    for k, v in vectors.items():\n",
    "        if k[1] == tag:\n",
    "            new_vectors[k] = v\n",
    "    \n",
    "    if dot_product:\n",
    "        s_dot_product(word, new_vectors, aux_path)\n",
    "                \n",
    "    if cosine:\n",
    "        s_cosine(word, new_vectors, aux_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f1576",
   "metadata": {},
   "source": [
    "#### Word associations from preprocessed text with sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e800b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_s = get_contexts_sents(vocabulary, preprocessed_text_sents)\n",
    "probs_s = get_vectors(vocabulary, contexts_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3766374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('empresa', 'n')\n",
      "Similarity estimations of \u001b[1m(empresa, n)\u001b[0m completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = (\"empresa\", \"n\")\n",
    "print(w)\n",
    "try:\n",
    "    similar_words(w, probs_s, aux_path = 'prob_vectors', dot_product = True, cosine = True)\n",
    "    print('Similarity estimations of \\033[1m(' + w[0] + ', ' + w[1] + ')\\033[0m completed\\n')\n",
    "except Exception as e:\n",
    "    print('An error has occured: ' + e + ' in word ' + w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
