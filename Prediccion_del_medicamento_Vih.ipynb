{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hEcHqthO8oB",
        "outputId": "30e37ec3-2e7e-4fdf-dd5f-d6766e82fed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark tar file downloaded successfully.\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark==1.4.2 in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq < /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Download the correct version of Apache Spark (3.5.4)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# Verify the download and extract the file\n",
        "import os\n",
        "if os.path.exists(\"spark-3.5.4-bin-hadoop3.tgz\"):\n",
        "    print(\"Spark tar file downloaded successfully.\")\n",
        "    # Extract the tar file\n",
        "    !tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "else:\n",
        "    print(\"Spark tar file not found. Download might have failed.\")\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "!pip install pyspark\n",
        "!pip install findspark==1.4.2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables (update SPARK_HOME if using a different version)\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create and test Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "try:\n",
        "    spark = SparkSession.builder.appName('Spark_1').getOrCreate()\n",
        "    print(\"Apache Spark version:\", spark.version)\n",
        "except Exception as e:\n",
        "    print(\"Error initializing Spark session:\", e)\n"
      ],
      "metadata": {
        "id": "RhrYjAuKPgFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c2366c-f73f-4a0f-ede1-c063e93d2047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apache Spark version: 3.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Covid Data Ingestion\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Cargar el archivo CSV subido\n",
        "filename = \"Personas_por_entidad_federativa_activas_en_sistema_con_Tratamiento_Antirretroviral_-_noviembre_2024.csv\"  # Reemplaza con la ruta del archivo subido\n",
        "dfi = spark.read.options(header='true', inferSchema='true').csv(filename)\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n",
        "dfi.show()\n",
        "\n",
        "# Mostrar el esquema del DataFrame\n",
        "dfi.printSchema()\n",
        "\n",
        "# Llenar valores nulos si es necesario\n",
        "dfi = dfi.fillna({\"consumo_arv_mensual\": 0, \"numero_pacientes\": 0})\n",
        "\n",
        "# Convertir 'corte_mes' a tipo entero\n",
        "dfi = dfi.withColumn(\"corte_mes\", F.col(\"corte_mes\").cast(\"int\"))\n",
        "\n",
        "# Crear una columna de fecha combinando corte_anio y corte_mes\n",
        "dfi = dfi.withColumn(\"fecha\", F.concat(F.col(\"corte_anio\"), F.lit(\"-\"), F.col(\"corte_mes\"), F.lit(\"-01\")))\n",
        "\n",
        "# Seleccionar las columnas necesarias\n",
        "dfi = dfi.select(\"nombre_medicamento\", \"consumo_arv_mensual\", \"numero_pacientes\", \"corte_anio\", \"corte_mes\", \"fecha\")\n",
        "from pyspark.sql.functions import mean, when, isnull\n",
        "\n",
        "# Calculate the mean of 'numero_pacientes', 'corte_anio', 'corte_mes'\n",
        "mean_numero_pacientes = dfi.select(mean(col('numero_pacientes'))).collect()[0][0]\n",
        "mean_corte_anio = dfi.select(mean(col('corte_anio'))).collect()[0][0]\n",
        "mean_corte_mes = dfi.select(mean(col('corte_mes'))).collect()[0][0]\n",
        "# Calculate the mean of 'corte_mes', handling potential None result\n",
        "mean_corte_mes_result = dfi.select(mean(col('corte_mes')).cast(\"integer\")).collect()[0][0]  # Force to integer\n",
        "mean_corte_mes = mean_corte_mes_result if mean_corte_mes_result is not None else 0 # Use 0 if None\n",
        "\n",
        "# Impute nulls using the calculated means or a constant value for categorical features\n",
        "dfi = dfi.fillna({\n",
        "    \"numero_pacientes\": mean_numero_pacientes,\n",
        "    \"corte_anio\": int(mean_corte_anio), # cast to int if 'corte_anio' is an integer column\n",
        "    \"corte_mes\": int(mean_corte_mes), # cast to int if 'corte_mes' is an integer column\n",
        "})\n",
        "# Convertir las columnas categóricas y numéricas a formato numérico usando VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[\"numero_pacientes\", \"corte_anio\", \"corte_mes\"], outputCol=\"features\", handleInvalid=\"skip\")\n",
        "dfi = assembler.transform(dfi)\n",
        "\n",
        "# Paso 2: Dividir los datos en conjunto de entrenamiento y prueba\n",
        "train_data, test_data = dfi.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "# Paso 3: Crear el modelo de regresión\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"consumo_arv_mensual\")\n",
        "\n",
        "# Paso 4: Entrenar el modelo\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Paso 5: Hacer predicciones\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Paso 6: Evaluar el modelo\n",
        "evaluator = RegressionEvaluator(labelCol=\"consumo_arv_mensual\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "# Paso 7: Predecir el medicamento más consumido en el siguiente mes\n",
        "medicamento_predicho = predictions \\\n",
        "    .groupBy(\"nombre_medicamento\") \\\n",
        "    .agg(F.sum(\"prediction\").alias(\"prediccion_total\")) \\\n",
        "    .orderBy(F.col(\"prediccion_total\").desc()) \\\n",
        "    .limit(1)\n",
        "\n",
        "# Mostrar el medicamento con la predicción más alta\n",
        "medicamento_predicho.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqLr7Q2BjizP",
        "outputId": "3187ce39-24ea-4e93-9c3d-9b5b4949f783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------------------+--------------------+-------------------+--------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|clave_medicamento|establecimiento_salud|      unidad_almacen|              corte|  nombre_medicamento|unidad_medida|corte_anio|   corte_mes|consumo_arv_mensual|numero_pacientes|\n",
            "+-----------------+---------------------+--------------------+-------------------+--------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|  010.000.4272.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|   ABACAVIR SOLUCION|           ml|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6203.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|BICTEGRAVIR 50 mg...|       Envase|      2024|Noviembre 25|              130.0|             130|\n",
            "|  010.000.5862.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5860.01|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    DARUNAVIR 400 mg|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.4289.01|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    DARUNAVIR 600 mg|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5861.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|     DARUNAVIR 75 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6098.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DARUNAVIR 800 mg,...|      Tableta|      2024|Noviembre 25|               13.0|              13|\n",
            "|  010.000.6318.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|  DOLUTEGRAVIR 10 MG|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6010.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|  DOLUTEGRAVIR 50 MG|      Tableta|      2024|Noviembre 25|                1.0|               1|\n",
            "|  010.000.6108.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DOLUTEGRAVIR 50 m...|      Tableta|      2024|Noviembre 25|                4.0|               4|\n",
            "|  010.000.7026.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DOLUTEGRAVIR 50 m...|      TABLETA|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.7106.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DOLUTEGRAVIR 5mg ...|      TABLETA|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6320.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|   DORAVIRINA 100 mg|      TABLETA|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.4370.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    EFAVIRENZ 600 mg|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5640.01|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|EFAVIRENZ 600 mg,...|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6162.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|EMTRICITABINA 200...|      Tableta|      2024|Noviembre 25|                1.0|               1|\n",
            "|  010.000.6163.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|EMTRICITABINA 200...|     TABLETAS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.4396.01|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|EMTRICITABINA 200...|      Tableta|      2024|Noviembre 25|               37.0|              37|\n",
            "|  010.000.4269.01|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|  ENFUVIRTIDA 108 mg|         Caja|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6074.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|   ETRAVIRINA 200 MG|      Tableta|      2024|Noviembre 25|                2.0|               2|\n",
            "+-----------------+---------------------+--------------------+-------------------+--------------------+-------------+----------+------------+-------------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- clave_medicamento: string (nullable = true)\n",
            " |-- establecimiento_salud: string (nullable = true)\n",
            " |-- unidad_almacen: string (nullable = true)\n",
            " |-- corte: timestamp (nullable = true)\n",
            " |-- nombre_medicamento: string (nullable = true)\n",
            " |-- unidad_medida: string (nullable = true)\n",
            " |-- corte_anio: integer (nullable = true)\n",
            " |-- corte_mes: string (nullable = true)\n",
            " |-- consumo_arv_mensual: double (nullable = true)\n",
            " |-- numero_pacientes: integer (nullable = true)\n",
            "\n",
            "RMSE: 2.4083264663238158\n",
            "+--------------------+-----------------+\n",
            "|  nombre_medicamento| prediccion_total|\n",
            "+--------------------+-----------------+\n",
            "|BICTEGRAVIR 50 mg...|36455.78795065143|\n",
            "+--------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar biblioteca para subir archivos en Google Colab\n",
        "from google.colab import files\n",
        "\n",
        "# Subir el archivo CSV\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verificar los archivos subidos\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Archivo subido: {filename}\")\n",
        "\n",
        "# Importar PySpark para trabajar con datos\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Covid Data Ingestion\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Cargar el archivo CSV subido\n",
        "dfi = spark.read.options(header='true', inferSchema='true').csv(filename)\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n",
        "dfi.show()\n",
        "\n",
        "# Mostrar el esquema del DataFrame\n",
        "dfi.printSchema()\n",
        "\n",
        "# Contar el número de filas en el DataFrame\n",
        "total_filas = dfi.count()\n",
        "print(f\"Total de filas: {total_filas}\")\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Paso 1: Preprocesamiento - Convertir las columnas en características y etiqueta\n",
        "# Aquí usamos 'consumo_arv_mensual' como la etiqueta (label) y otras columnas como características\n",
        "\n",
        "# Crear un DataFrame con las columnas relevantes\n",
        "dfi = dfi.select(\"nombre_medicamento\", \"consumo_arv_mensual\", \"numero_pacientes\", \"corte_anio\", \"corte_mes\")\n",
        "\n",
        "# Llenar valores nulos si es necesario\n",
        "dfi = dfi.fillna({\"consumo_arv_mensual\": 0, \"numero_pacientes\": 0})\n",
        "dfi = dfi.withColumn(\"corte_mes\", F.col(\"corte_mes\").cast(\"int\"))\n",
        "dfi = dfi.withColumn(\"corte_mes\", col(\"corte_mes\").cast(\"int\"))\n",
        "# Crear una columna de fecha combinando corte_anio y corte_mes\n",
        "dfi = dfi.withColumn(\"fecha\", F.concat(F.col(\"corte_anio\"), F.lit(\"-\"), F.col(\"corte_mes\"), F.lit(\"-01\")))\n",
        "\n",
        "\n",
        "# Convertir las columnas categóricas y numéricas a formato numérico\n",
        "# Aquí combinamos 'numero_pacientes' y 'fecha' como características\n",
        "\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"numero_pacientes\", \"corte_anio\", \"corte_mes\"], outputCol=\"features\")\n",
        "dfi = assembler.transform(dfi)\n",
        "\n",
        "# Paso 2: Dividir los datos en conjunto de entrenamiento y prueba\n",
        "train_data, test_data = dfi.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "# Paso 3: Crear el modelo de regresión\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"consumo_arv_mensual\")\n",
        "\n",
        "# Paso 4: Entrenar el modelo\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Paso 5: Hacer predicciones\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Paso 6: Evaluar el modelo\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"consumo_arv_mensual\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "# Paso 7: Predecir el medicamento más consumido en el siguiente mes\n",
        "# Ordenar las predicciones por el consumo mensual y seleccionar el medicamento con mayor consumo\n",
        "\n",
        "medicamento_predicho = predictions \\\n",
        "    .groupBy(\"nombre_medicamento\") \\\n",
        "    .agg(F.sum(\"prediction\").alias(\"prediccion_total\")) \\\n",
        "    .orderBy(F.col(\"prediccion_total\").desc()) \\\n",
        "    .limit(1)\n",
        "\n",
        "# Mostrar el medicamento con la predicción más alta\n",
        "medicamento_predicho.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7WezgNWAPtPY",
        "outputId": "3835abf7-364e-43da-a759-87939d55c0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1f156eb7-f8f7-4f73-a3c9-3f02235e2851\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1f156eb7-f8f7-4f73-a3c9-3f02235e2851\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Personas_por_entidad_federativa_activas_en_sistema_con_Tratamiento_Antirretroviral_-_noviembre_2024.csv to Personas_por_entidad_federativa_activas_en_sistema_con_Tratamiento_Antirretroviral_-_noviembre_2024 (2).csv\n",
            "Archivo subido: Personas_por_entidad_federativa_activas_en_sistema_con_Tratamiento_Antirretroviral_-_noviembre_2024 (2).csv\n",
            "+-----------------+---------------------+--------------------+-------------------+--------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|clave_medicamento|establecimiento_salud|      unidad_almacen|              corte|  nombre_medicamento|unidad_medida|corte_anio|   corte_mes|consumo_arv_mensual|numero_pacientes|\n",
            "+-----------------+---------------------+--------------------+-------------------+--------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|  010.000.4272.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|   ABACAVIR SOLUCION|           ml|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6203.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|BICTEGRAVIR 50 mg...|       Envase|      2024|Noviembre 25|              130.0|             130|\n",
            "|  010.000.5862.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5860.01|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    DARUNAVIR 400 mg|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.4289.01|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    DARUNAVIR 600 mg|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5861.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|     DARUNAVIR 75 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6098.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DARUNAVIR 800 mg,...|      Tableta|      2024|Noviembre 25|               13.0|              13|\n",
            "|  010.000.6318.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|  DOLUTEGRAVIR 10 MG|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6010.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|  DOLUTEGRAVIR 50 MG|      Tableta|      2024|Noviembre 25|                1.0|               1|\n",
            "|  010.000.6108.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DOLUTEGRAVIR 50 m...|      Tableta|      2024|Noviembre 25|                4.0|               4|\n",
            "|  010.000.7026.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DOLUTEGRAVIR 50 m...|      TABLETA|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.7106.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DOLUTEGRAVIR 5mg ...|      TABLETA|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6320.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|   DORAVIRINA 100 mg|      TABLETA|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.4370.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    EFAVIRENZ 600 mg|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5640.01|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|EFAVIRENZ 600 mg,...|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6162.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|EMTRICITABINA 200...|      Tableta|      2024|Noviembre 25|                1.0|               1|\n",
            "|  010.000.6163.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|EMTRICITABINA 200...|     TABLETAS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.4396.01|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|EMTRICITABINA 200...|      Tableta|      2024|Noviembre 25|               37.0|              37|\n",
            "|  010.000.4269.01|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|  ENFUVIRTIDA 108 mg|         Caja|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6074.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|   ETRAVIRINA 200 MG|      Tableta|      2024|Noviembre 25|                2.0|               2|\n",
            "+-----------------+---------------------+--------------------+-------------------+--------------------+-------------+----------+------------+-------------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- clave_medicamento: string (nullable = true)\n",
            " |-- establecimiento_salud: string (nullable = true)\n",
            " |-- unidad_almacen: string (nullable = true)\n",
            " |-- corte: timestamp (nullable = true)\n",
            " |-- nombre_medicamento: string (nullable = true)\n",
            " |-- unidad_medida: string (nullable = true)\n",
            " |-- corte_anio: integer (nullable = true)\n",
            " |-- corte_mes: string (nullable = true)\n",
            " |-- consumo_arv_mensual: double (nullable = true)\n",
            " |-- numero_pacientes: integer (nullable = true)\n",
            "\n",
            "Total de filas: 8100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o1213.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 167.0 failed 1 times, most recent failure: Lost task 0.0 in stage 167.0 (TID 150) (a5517985eee2 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4112/1142805557`: (struct<numero_pacientes_double_VectorAssembler_f247711c1a1b:double,corte_anio_double_VectorAssembler_f247711c1a1b:double,corte_mes_double_VectorAssembler_f247711c1a1b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4112/1142805557`: (struct<numero_pacientes_double_VectorAssembler_f247711c1a1b:double,corte_anio_double_VectorAssembler_f247711c1a1b:double,corte_mes_double_VectorAssembler_f247711c1a1b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-16306d27552a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Paso 4: Entrenar el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Paso 5: Hacer predicciones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.4-bin-hadoop3/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/content/spark-3.5.4-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.4-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.4-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1213.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 167.0 failed 1 times, most recent failure: Lost task 0.0 in stage 167.0 (TID 150) (a5517985eee2 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4112/1142805557`: (struct<numero_pacientes_double_VectorAssembler_f247711c1a1b:double,corte_anio_double_VectorAssembler_f247711c1a1b:double,corte_mes_double_VectorAssembler_f247711c1a1b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4112/1142805557`: (struct<numero_pacientes_double_VectorAssembler_f247711c1a1b:double,corte_anio_double_VectorAssembler_f247711c1a1b:double,corte_mes_double_VectorAssembler_f247711c1a1b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Covid Data Ingestion\") \\\n",
        "    .config(\"spark.driver.memory\", \"30g\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "jas1j8QjQkmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar las bibliotecas necesarias\n",
        "!pip install ipython-sql\n",
        "!pip install sqlalchemy\n",
        "\n",
        "# Cargar la extensión SQL\n",
        "%load_ext sql\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6urvhdGSQo97",
        "outputId": "ed7decce-b5c3-4d2f-da0e-535475c7bbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-sql in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from ipython-sql) (3.12.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from ipython-sql) (7.34.0)\n",
            "Requirement already satisfied: sqlalchemy>=2.0 in /usr/local/lib/python3.11/dist-packages (from ipython-sql) (2.0.37)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.11/dist-packages (from ipython-sql) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from ipython-sql) (1.17.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=2.0->ipython-sql) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=2.0->ipython-sql) (4.12.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-sql) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-sql)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-sql) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-sql) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-sql) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-sql) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-sql) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-sql) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-sql) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->ipython-sql) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (2.0.37)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una conexión a una base de datos SQLite\n",
        "%sql sqlite:///my_database.db\n",
        "# Contar el número de filas\n",
        "total_filas = dfi.count()\n",
        "print(f\"Total de filas: {total_filas}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41PwPlNqQtP7",
        "outputId": "81a5cf83-a3c2-40f0-c44e-54a385c769fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de filas: 8100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener estadísticas descriptivas\n",
        "dfi.describe().show()"
      ],
      "metadata": {
        "id": "2YtFUXLsQ8J3",
        "outputId": "a03334e0-b910-4079-fdf5-688a2481e6ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+---------------------+--------------------+--------------------+-------------+----------+------------+-------------------+------------------+\n",
            "|summary|clave_medicamento|establecimiento_salud|      unidad_almacen|  nombre_medicamento|unidad_medida|corte_anio|   corte_mes|consumo_arv_mensual|  numero_pacientes|\n",
            "+-------+-----------------+---------------------+--------------------+--------------------+-------------+----------+------------+-------------------+------------------+\n",
            "|  count|             8100|                 8100|                8100|                8100|         8100|      8100|        8100|               8100|              8100|\n",
            "|   mean|             NULL|                 NULL|                NULL|                NULL|         NULL|    2024.0|        NULL|  21.07088888888889|20.720123456790123|\n",
            "| stddev|             NULL|                 NULL|                NULL|                NULL|         NULL|       0.0|        NULL| 203.84898066664354|203.45871785895133|\n",
            "|    min|  010.000.4268.00|       AGUASCALIENTES|Almacen  Estatal ...|ABACAVIR 600 mg, ...|     CAPSULAS|      2024|Noviembre 25|                0.0|                 0|\n",
            "|    max|  010.000.7106.00|            ZACATECAS|Unidad-Privados d...|ZIDOVUDINA SOLUCI...|           ml|      2024|Noviembre 25|            11261.0|             11248|\n",
            "+-------+-----------------+---------------------+--------------------+--------------------+-------------+----------+------------+-------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar las filas donde 'nombre_medicamento' sea 'DARUNAVIR 150 mg'\n",
        "dfi.filter(dfi['nombre_medicamento'] == \"DARUNAVIR 150 mg\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYpcvck_O0t2",
        "outputId": "875eeb4a-2493-4732-bebf-f79a98597059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------------------+--------------------+-------------------+------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|clave_medicamento|establecimiento_salud|      unidad_almacen|              corte|nombre_medicamento|unidad_medida|corte_anio|   corte_mes|consumo_arv_mensual|numero_pacientes|\n",
            "+-----------------+---------------------+--------------------+-------------------+------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|  010.000.5862.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|              YUCATAN|   CAPASITS - M�rida|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|              YUCATAN|CAPASITS - Vallad...|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|              YUCATAN|Almac�n Estatal Y...|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|             VERACRUZ|CAPASITS - Poza Rica|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|             VERACRUZ| CAPASITS - Veracruz|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|             VERACRUZ|Almacen  Estatal ...|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|             TLAXCALA|Hospital General ...|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|             TLAXCALA|Almacen  Estatal ...|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|           TAMAULIPAS|CAPASITS - Matamoros|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|           TAMAULIPAS|  CAPASITS - Reynosa|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|           TAMAULIPAS|Hospital Civil Vi...|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|           TAMAULIPAS|Almacen  Estatal ...|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|              TABASCO| CAPASITS - Cardenas|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|              TABASCO|CAPASITS - Macuspana|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|              TABASCO|CAPASITS - Tenosique|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|              TABASCO|CAPASITS - Villah...|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|              TABASCO|      CESSA Gaviotas|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5862.00|              TABASCO|CESSA Maximiliano...|2024-11-25 06:00:00|  DARUNAVIR 150 mg|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "+-----------------+---------------------+--------------------+-------------------+------------------+-------------+----------+------------+-------------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper, trim, col\n",
        "\n",
        "# Convertir a mayúsculas y eliminar espacios en blanco\n",
        "dfi = dfi.withColumn(\"nombre_medicamento\", upper(trim(col(\"nombre_medicamento\"))))\n",
        "\n"
      ],
      "metadata": {
        "id": "FaPcXepdRP07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras filas y el esquema limpio\n",
        "dfi.show()\n",
        "dfi.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxryyXDPRt6B",
        "outputId": "9e83fa3b-84ca-42de-c9da-6e834ddf9962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------------------+--------------------+-------------------+--------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|clave_medicamento|establecimiento_salud|      unidad_almacen|              corte|  nombre_medicamento|unidad_medida|corte_anio|   corte_mes|consumo_arv_mensual|numero_pacientes|\n",
            "+-----------------+---------------------+--------------------+-------------------+--------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|  010.000.4272.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|   ABACAVIR SOLUCION|           ml|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6203.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|BICTEGRAVIR 50 MG...|       Envase|      2024|Noviembre 25|              130.0|             130|\n",
            "|  010.000.5862.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    DARUNAVIR 150 MG|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5860.01|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    DARUNAVIR 400 MG|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.4289.01|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    DARUNAVIR 600 MG|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5861.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|     DARUNAVIR 75 MG|  COMPRIMIDOS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6098.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DARUNAVIR 800 MG,...|      Tableta|      2024|Noviembre 25|               13.0|              13|\n",
            "|  010.000.6318.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|  DOLUTEGRAVIR 10 MG|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6010.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|  DOLUTEGRAVIR 50 MG|      Tableta|      2024|Noviembre 25|                1.0|               1|\n",
            "|  010.000.6108.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DOLUTEGRAVIR 50 M...|      Tableta|      2024|Noviembre 25|                4.0|               4|\n",
            "|  010.000.7026.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DOLUTEGRAVIR 50 M...|      TABLETA|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.7106.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|DOLUTEGRAVIR 5MG ...|      TABLETA|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6320.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|   DORAVIRINA 100 MG|      TABLETA|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.4370.00|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|    EFAVIRENZ 600 MG|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.5640.01|            ZACATECAS|CAPASITS - Fresnillo|2024-11-25 06:00:00|EFAVIRENZ 600 MG,...|      Tableta|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6162.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|EMTRICITABINA 200...|      Tableta|      2024|Noviembre 25|                1.0|               1|\n",
            "|  010.000.6163.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|EMTRICITABINA 200...|     TABLETAS|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.4396.01|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|EMTRICITABINA 200...|      Tableta|      2024|Noviembre 25|               37.0|              37|\n",
            "|  010.000.4269.01|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|  ENFUVIRTIDA 108 MG|         Caja|      2024|Noviembre 25|                0.0|               0|\n",
            "|  010.000.6074.00|            ZACATECAS|CAPASITS - Guadalupe|2024-11-25 06:00:00|   ETRAVIRINA 200 MG|      Tableta|      2024|Noviembre 25|                2.0|               2|\n",
            "+-----------------+---------------------+--------------------+-------------------+--------------------+-------------+----------+------------+-------------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- clave_medicamento: string (nullable = true)\n",
            " |-- establecimiento_salud: string (nullable = true)\n",
            " |-- unidad_almacen: string (nullable = true)\n",
            " |-- corte: timestamp (nullable = true)\n",
            " |-- nombre_medicamento: string (nullable = true)\n",
            " |-- unidad_medida: string (nullable = true)\n",
            " |-- corte_anio: integer (nullable = true)\n",
            " |-- corte_mes: string (nullable = true)\n",
            " |-- consumo_arv_mensual: double (nullable = true)\n",
            " |-- numero_pacientes: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importar funciones necesarias de PySpark\n",
        "from pyspark.sql.functions import col, concat_ws, to_date, lit\n",
        "\n",
        "# Crear una nueva columna 'corte' combinando corte_anio y corte_mes en formato 'yyyy-MM'\n",
        "dfi = dfi.withColumn(\"corte\", to_date(concat_ws(\"-\", col(\"corte_anio\"), col(\"corte_mes\"), lit(\"01\")), \"yyyy-MM-dd\"))\n",
        "\n",
        "# Verificar el nuevo esquema\n",
        "dfi.printSchema()\n",
        "\n",
        "# Crear una vista temporal para consultas SQL\n",
        "dfi.createOrReplaceTempView(\"tabla_medicamentos\")\n",
        "\n",
        "# Mostrar una consulta simple para verificar los datos\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    clave_medicamento,\n",
        "    establecimiento_salud,\n",
        "    unidad_almacen,\n",
        "    corte,\n",
        "    nombre_medicamento,\n",
        "    unidad_medida,\n",
        "    consumo_arv_mensual,\n",
        "    numero_pacientes\n",
        "FROM tabla_medicamentos\n",
        "LIMIT 10\n",
        "\"\"\").show()\n",
        "\n",
        "# Confirmación de la creación de la vista temporal\n",
        "print(\"Vista SQL 'tabla_medicamentos' creada exitosamente.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12BsG7f1TT6Y",
        "outputId": "bfed10de-a930-41ad-a7c2-6fad779bb233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- clave_medicamento: string (nullable = true)\n",
            " |-- establecimiento_salud: string (nullable = true)\n",
            " |-- unidad_almacen: string (nullable = true)\n",
            " |-- corte: date (nullable = true)\n",
            " |-- nombre_medicamento: string (nullable = true)\n",
            " |-- unidad_medida: string (nullable = true)\n",
            " |-- corte_anio: integer (nullable = true)\n",
            " |-- corte_mes: string (nullable = true)\n",
            " |-- consumo_arv_mensual: double (nullable = true)\n",
            " |-- numero_pacientes: integer (nullable = true)\n",
            "\n",
            "+-----------------+---------------------+--------------------+-----+--------------------+-------------+-------------------+----------------+\n",
            "|clave_medicamento|establecimiento_salud|      unidad_almacen|corte|  nombre_medicamento|unidad_medida|consumo_arv_mensual|numero_pacientes|\n",
            "+-----------------+---------------------+--------------------+-----+--------------------+-------------+-------------------+----------------+\n",
            "|  010.000.4272.00|            ZACATECAS|CAPASITS - Fresnillo| NULL|   ABACAVIR SOLUCION|           ml|                0.0|               0|\n",
            "|  010.000.6203.00|            ZACATECAS|CAPASITS - Fresnillo| NULL|BICTEGRAVIR 50 MG...|       Envase|              130.0|             130|\n",
            "|  010.000.5862.00|            ZACATECAS|CAPASITS - Fresnillo| NULL|    DARUNAVIR 150 MG|  COMPRIMIDOS|                0.0|               0|\n",
            "|  010.000.5860.01|            ZACATECAS|CAPASITS - Fresnillo| NULL|    DARUNAVIR 400 MG|      Tableta|                0.0|               0|\n",
            "|  010.000.4289.01|            ZACATECAS|CAPASITS - Fresnillo| NULL|    DARUNAVIR 600 MG|      Tableta|                0.0|               0|\n",
            "|  010.000.5861.00|            ZACATECAS|CAPASITS - Fresnillo| NULL|     DARUNAVIR 75 MG|  COMPRIMIDOS|                0.0|               0|\n",
            "|  010.000.6098.00|            ZACATECAS|CAPASITS - Fresnillo| NULL|DARUNAVIR 800 MG,...|      Tableta|               13.0|              13|\n",
            "|  010.000.6318.00|            ZACATECAS|CAPASITS - Fresnillo| NULL|  DOLUTEGRAVIR 10 MG|      Tableta|                0.0|               0|\n",
            "|  010.000.6010.00|            ZACATECAS|CAPASITS - Fresnillo| NULL|  DOLUTEGRAVIR 50 MG|      Tableta|                1.0|               1|\n",
            "|  010.000.6108.00|            ZACATECAS|CAPASITS - Fresnillo| NULL|DOLUTEGRAVIR 50 M...|      Tableta|                4.0|               4|\n",
            "+-----------------+---------------------+--------------------+-----+--------------------+-------------+-------------------+----------------+\n",
            "\n",
            "Vista SQL 'tabla_medicamentos' creada exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análisis basado en el DataFrame dfi\n",
        "a. Identificación de Dimensiones y Hechos\n",
        "Dimensiones:\n",
        "\n",
        "clave_medicamento: Identificador único del medicamento.\n",
        "establecimiento_salud: Centro de salud asociado al consumo del medicamento.\n",
        "unidad_almacen: Almacén que distribuye los medicamentos.\n",
        "nombre_medicamento: Nombre comercial o técnico del medicamento.\n",
        "unidad_medida: Unidad en la que se mide el consumo (tabletas, frascos, etc.).\n",
        "corte: Fecha de corte en formato yyyy-MM-dd que combina corte_anio y corte_mes.\n",
        "\n",
        "Hechos:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "consumo_arv_mensual: Cantidad total de medicamento consumido en un mes.\n",
        "numero_pacientes: Número de pacientes que recibieron el medicamento en el periodo indicado."
      ],
      "metadata": {
        "id": "APPvBXxed6V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una vista temporal\n",
        "dfi.createOrReplaceTempView(\"tabla_medicamentos\")\n",
        "\n",
        "# Consulta SQL para contar valores nulos por columna\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    SUM(CASE WHEN clave_medicamento IS NULL THEN 1 ELSE 0 END) AS nulos_clave_medicamento,\n",
        "    SUM(CASE WHEN establecimiento_salud IS NULL THEN 1 ELSE 0 END) AS nulos_establecimiento_salud,\n",
        "    SUM(CASE WHEN unidad_almacen IS NULL THEN 1 ELSE 0 END) AS nulos_unidad_almacen,\n",
        "    SUM(CASE WHEN nombre_medicamento IS NULL THEN 1 ELSE 0 END) AS nulos_nombre_medicamento,\n",
        "    SUM(CASE WHEN unidad_medida IS NULL THEN 1 ELSE 0 END) AS nulos_unidad_medida,\n",
        "    SUM(CASE WHEN corte_anio IS NULL THEN 1 ELSE 0 END) AS nulos_corte_anio,\n",
        "    SUM(CASE WHEN corte_mes IS NULL THEN 1 ELSE 0 END) AS nulos_corte_mes,\n",
        "    SUM(CASE WHEN consumo_arv_mensual IS NULL THEN 1 ELSE 0 END) AS nulos_consumo_arv_mensual,\n",
        "    SUM(CASE WHEN numero_pacientes IS NULL THEN 1 ELSE 0 END) AS nulos_numero_pacientes\n",
        "FROM tabla_medicamentos\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usXj3g_BX1gu",
        "outputId": "60266c25-d548-4c94-b8b2-3aa229d54668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+---------------------------+--------------------+------------------------+-------------------+----------------+---------------+-------------------------+----------------------+\n",
            "|nulos_clave_medicamento|nulos_establecimiento_salud|nulos_unidad_almacen|nulos_nombre_medicamento|nulos_unidad_medida|nulos_corte_anio|nulos_corte_mes|nulos_consumo_arv_mensual|nulos_numero_pacientes|\n",
            "+-----------------------+---------------------------+--------------------+------------------------+-------------------+----------------+---------------+-------------------------+----------------------+\n",
            "|                      0|                          0|                   0|                       0|                  0|               0|              0|                        0|                     0|\n",
            "+-----------------------+---------------------------+--------------------+------------------------+-------------------+----------------+---------------+-------------------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Jerarquías en las Dimensiones\n",
        "Dimensiones con Jerarquías:\n",
        "\n",
        "Jerarquía Temporal (en la dimensión corte):\n",
        "\n",
        "Nivel 1: Año (corte_anio).\n",
        "Nivel 2: Mes (corte_mes).\n",
        "Nivel 3: Fecha (corte).\n",
        "Jerarquía de Establecimiento:\n",
        "\n",
        "Nivel 1: Estado (puede derivarse si existe una columna relacionada o información geográfica del establecimiento).\n",
        "Nivel 2: Municipio (asociado al estado, si se tienen los datos).\n",
        "Nivel 3: Nombre del establecimiento (establecimiento_salud).\n",
        "Jerarquía del Medicamento:\n",
        "\n",
        "Nivel 1: Categoría del medicamento (por ejemplo, antirretrovirales).\n",
        "Nivel 2: Nombre del principio activo (puede derivarse de nombre_medicamento si se tiene un catálogo).\n",
        "Nivel 3: Nombre del medicamento específico (nombre_medicamento).\n"
      ],
      "metadata": {
        "id": "x245aV-zfUUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consulta para obtener la jerarquía de medicamentos (categoría, principio activo y nombre del medicamento)\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    SUBSTRING(nombre_medicamento, 1, 15) AS categoria,\n",
        "    nombre_medicamento AS principio_activo,\n",
        "    nombre_medicamento\n",
        "FROM tabla_medicamentos\n",
        "GROUP BY categoria, principio_activo, nombre_medicamento\n",
        "ORDER BY categoria\n",
        "\"\"\").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF7oDS5jf2od",
        "outputId": "d14366d5-d9ae-49bf-a451-bddd4eba4804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------------------+\n",
            "|      categoria|    principio_activo|  nombre_medicamento|\n",
            "+---------------+--------------------+--------------------+\n",
            "|ABACAVIR 600 MG|ABACAVIR 600 MG, ...|ABACAVIR 600 MG, ...|\n",
            "|ABACAVIR SOLUCI|   ABACAVIR SOLUCION|   ABACAVIR SOLUCION|\n",
            "|ABACAVIR TABLET|    ABACAVIR TABLETA|    ABACAVIR TABLETA|\n",
            "|BICTEGRAVIR 50 |BICTEGRAVIR 50 MG...|BICTEGRAVIR 50 MG...|\n",
            "|DARUNAVIR 150 M|    DARUNAVIR 150 MG|    DARUNAVIR 150 MG|\n",
            "|DARUNAVIR 400 M|    DARUNAVIR 400 MG|    DARUNAVIR 400 MG|\n",
            "|DARUNAVIR 600 M|    DARUNAVIR 600 MG|    DARUNAVIR 600 MG|\n",
            "|DARUNAVIR 75 MG|     DARUNAVIR 75 MG|     DARUNAVIR 75 MG|\n",
            "|DARUNAVIR 800 M|DARUNAVIR 800 MG,...|DARUNAVIR 800 MG,...|\n",
            "|DOLUTEGRAVIR 10|  DOLUTEGRAVIR 10 MG|  DOLUTEGRAVIR 10 MG|\n",
            "|DOLUTEGRAVIR 50|DOLUTEGRAVIR 50 M...|DOLUTEGRAVIR 50 M...|\n",
            "|DOLUTEGRAVIR 50|  DOLUTEGRAVIR 50 MG|  DOLUTEGRAVIR 50 MG|\n",
            "|DOLUTEGRAVIR 50|DOLUTEGRAVIR 50 M...|DOLUTEGRAVIR 50 M...|\n",
            "|DOLUTEGRAVIR 5M|DOLUTEGRAVIR 5MG ...|DOLUTEGRAVIR 5MG ...|\n",
            "|DORAVIRINA 100 |   DORAVIRINA 100 MG|   DORAVIRINA 100 MG|\n",
            "|EFAVIRENZ 600 M|    EFAVIRENZ 600 MG|    EFAVIRENZ 600 MG|\n",
            "|EFAVIRENZ 600 M|EFAVIRENZ 600 MG,...|EFAVIRENZ 600 MG,...|\n",
            "|EMTRICITABINA 2|EMTRICITABINA 200...|EMTRICITABINA 200...|\n",
            "|EMTRICITABINA 2|EMTRICITABINA 200...|EMTRICITABINA 200...|\n",
            "|EMTRICITABINA 2|EMTRICITABINA 200...|EMTRICITABINA 200...|\n",
            "+---------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consulta para obtener la jerarquía de establecimiento (estado, municipio y nombre del establecimiento)\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    SUBSTRING(establecimiento_salud, 1, 10) AS estado,  -- Ajustar según datos disponibles\n",
        "    establecimiento_salud AS municipio,  -- Asegúrate de tener la columna de municipio\n",
        "    establecimiento_salud\n",
        "FROM tabla_medicamentos\n",
        "GROUP BY estado, municipio, establecimiento_salud\n",
        "ORDER BY estado, municipio\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTUhtxcshS7O",
        "outputId": "2ce14e1c-26d5-4f9e-a31a-402ab7025e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+---------------------+\n",
            "|    estado|           municipio|establecimiento_salud|\n",
            "+----------+--------------------+---------------------+\n",
            "|AGUASCALIE|      AGUASCALIENTES|       AGUASCALIENTES|\n",
            "|BAJA CALIF|     BAJA CALIFORNIA|      BAJA CALIFORNIA|\n",
            "|BAJA CALIF| BAJA CALIFORNIA SUR|  BAJA CALIFORNIA SUR|\n",
            "|  CAMPECHE|            CAMPECHE|             CAMPECHE|\n",
            "|   CHIAPAS|             CHIAPAS|              CHIAPAS|\n",
            "| CHIHUAHUA|           CHIHUAHUA|            CHIHUAHUA|\n",
            "|CIUDAD DE |    CIUDAD DE MEXICO|     CIUDAD DE MEXICO|\n",
            "|CLINICA DE|CLINICA DE INMUNO...| CLINICA DE INMUNO...|\n",
            "|  COAHUILA|            COAHUILA|             COAHUILA|\n",
            "|    COLIMA|              COLIMA|               COLIMA|\n",
            "|   DURANGO|             DURANGO|              DURANGO|\n",
            "|GUANAJUATO|          GUANAJUATO|           GUANAJUATO|\n",
            "|  GUERRERO|            GUERRERO|             GUERRERO|\n",
            "|   HIDALGO|             HIDALGO|              HIDALGO|\n",
            "|HOSPITAL G|HOSPITAL GENERAL ...| HOSPITAL GENERAL ...|\n",
            "|HOSPITAL I|HOSPITAL INFANTIL...| HOSPITAL INFANTIL...|\n",
            "|INSTITUTO |INSTITUTO NACIONA...| INSTITUTO NACIONA...|\n",
            "|INSTITUTO |INSTITUTO NACIONA...| INSTITUTO NACIONA...|\n",
            "|INSTITUTO |INSTITUTO NACIONA...| INSTITUTO NACIONA...|\n",
            "|INSTITUTO |INSTITUTO NACIONA...| INSTITUTO NACIONA...|\n",
            "+----------+--------------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Crear la sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"Clustering Example\").getOrCreate()\n",
        "\n",
        "# Suponiendo que tienes el DataFrame 'dfi' cargado con las columnas 'consumo_arv_mensual' y 'numero_pacientes'\n",
        "dfi = dfi.select(\"consumo_arv_mensual\", \"numero_pacientes\")\n",
        "\n",
        "# Usar VectorAssembler para combinar las características en una sola columna\n",
        "assembler = VectorAssembler(inputCols=[\"consumo_arv_mensual\", \"numero_pacientes\"], outputCol=\"features\")\n",
        "dfi = assembler.transform(dfi)\n",
        "\n",
        "# Crear el modelo de K-means (con k=3 clústeres)\n",
        "kmeans = KMeans().setK(3).setSeed(1)\n",
        "model = kmeans.fit(dfi)\n",
        "\n",
        "# Predecir el clúster al que pertenece cada punto de datos\n",
        "predictions = model.transform(dfi)\n",
        "\n",
        "# Mostrar las predicciones\n",
        "predictions.select(\"consumo_arv_mensual\", \"numero_pacientes\", \"prediction\").show()\n",
        "\n",
        "# Ver los centros de los clústeres\n",
        "centers = model.clusterCenters()\n",
        "print(\"Centros de los clústeres:\")\n",
        "for center in centers:\n",
        "    print(center)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YikuVVp4h64N",
        "outputId": "cb27b8a6-730f-4198-e7d9-b03f7d478655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------------+----------+\n",
            "|consumo_arv_mensual|numero_pacientes|prediction|\n",
            "+-------------------+----------------+----------+\n",
            "|                0.0|               0|         0|\n",
            "|              130.0|             130|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|               13.0|              13|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                1.0|               1|         0|\n",
            "|                4.0|               4|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                1.0|               1|         0|\n",
            "|                0.0|               0|         0|\n",
            "|               37.0|              37|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                2.0|               2|         0|\n",
            "+-------------------+----------------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Centros de los clústeres:\n",
            "[8.4101952  8.09411911]\n",
            "[6839.33333333 6831.66666667]\n",
            "[1528.01851852 1522.90740741]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ve29wlUgXpob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import GaussianMixture\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "\n",
        "# Crear la sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"Clustering Example\").getOrCreate()\n",
        "\n",
        "# Suponiendo que tienes el DataFrame 'dfi' cargado con las columnas 'consumo_arv_mensual' y 'numero_pacientes'\n",
        "dfi = dfi.select(\"consumo_arv_mensual\", \"numero_pacientes\")\n",
        "\n",
        "# Usar VectorAssembler para combinar las características en una sola columna\n",
        "assembler = VectorAssembler(inputCols=[\"consumo_arv_mensual\", \"numero_pacientes\"], outputCol=\"features\")\n",
        "dfi = assembler.transform(dfi)\n",
        "\n",
        "# Crear el modelo de GMM (con k=3 clústeres)\n",
        "gmm = GaussianMixture().setK(3).setSeed(1)\n",
        "model = gmm.fit(dfi)\n",
        "\n",
        "# Predecir el clúster al que pertenece cada punto de datos\n",
        "predictions = model.transform(dfi)\n",
        "\n",
        "# Mostrar las predicciones\n",
        "predictions.select(\"consumo_arv_mensual\", \"numero_pacientes\", \"prediction\").show()\n",
        "\n",
        "# Ver las estadísticas de los clústeres\n",
        "summary = model.summary\n",
        "print(\"Log-likelihood:\", summary.logLikelihood)\n",
        "\n",
        "# Número de puntos de datos\n",
        "n = dfi.count()\n",
        "\n",
        "# Número de características\n",
        "d = len(dfi.columns) - 1  # menos la columna 'features'\n",
        "\n",
        "# Número de parámetros en GMM: k*(d + d(d+1)/2) - 1\n",
        "k = 3  # número de clústeres\n",
        "num_parameters = k * (d + d * (d + 1) / 2) - 1\n",
        "\n",
        "# Log-likelihood\n",
        "log_likelihood = summary.logLikelihood\n",
        "\n",
        "# Calcular AIC y BIC\n",
        "aic = 2 * num_parameters - 2 * log_likelihood\n",
        "bic = np.log(n) * num_parameters - 2 * log_likelihood\n",
        "\n",
        "print(\"AIC:\", aic)\n",
        "print(\"BIC:\", bic)\n",
        "\n",
        "# Ver los centros de los clústeres\n",
        "centers = model.gaussiansDF.select(\"mean\").collect()\n",
        "print(\"Centros de los clústeres:\")\n",
        "for center in centers:\n",
        "    print(center)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpfit9YXT3dG",
        "outputId": "81632106-dc26-4ff4-80b2-936d6286a45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------------+----------+\n",
            "|consumo_arv_mensual|numero_pacientes|prediction|\n",
            "+-------------------+----------------+----------+\n",
            "|                0.0|               0|         0|\n",
            "|              130.0|             130|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|               13.0|              13|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                1.0|               1|         0|\n",
            "|                4.0|               4|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                1.0|               1|         0|\n",
            "|                0.0|               0|         0|\n",
            "|               37.0|              37|         0|\n",
            "|                0.0|               0|         0|\n",
            "|                2.0|               2|         0|\n",
            "+-------------------+----------------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Log-likelihood: -36322.72160866712\n",
            "AIC: 72673.44321733424\n",
            "BIC: 72771.43788810348\n",
            "Centros de los clústeres:\n",
            "Row(mean=DenseVector([4.3484, 4.3477]))\n",
            "Row(mean=DenseVector([688.8159, 671.5329]))\n",
            "Row(mean=DenseVector([155.5215, 153.0806]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar todos los registros de la tabla temporal 'tabla_medicamentos'\n",
        "spark.sql(\"SELECT * FROM tabla_medicamentos\").show(truncate=False)\n",
        "\n",
        "# Si prefieres un límite de filas para no llenar la consola:\n",
        "spark.sql(\"SELECT * FROM tabla_medicamentos LIMIT 10\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA9eIh-xXrzg",
        "outputId": "ea15253d-c9ef-4f0b-d8c8-9c8a8e06c2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------------------+--------------------+-----+----------------------------------------------------------------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|clave_medicamento|establecimiento_salud|unidad_almacen      |corte|nombre_medicamento                                                          |unidad_medida|corte_anio|corte_mes   |consumo_arv_mensual|numero_pacientes|\n",
            "+-----------------+---------------------+--------------------+-----+----------------------------------------------------------------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|010.000.4272.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |ABACAVIR SOLUCION                                                           |ml           |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.6203.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |BICTEGRAVIR 50 MG, EMTRICITABINA 200 MG, TENOFOVIR ALAFENAMIDA 25 MG        |Envase       |2024      |Noviembre 25|130.0              |130             |\n",
            "|010.000.5862.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 150 MG                                                            |COMPRIMIDOS  |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.5860.01  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 400 MG                                                            |Tableta      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.4289.01  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 600 MG                                                            |Tableta      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.5861.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 75 MG                                                             |COMPRIMIDOS  |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.6098.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 800 MG, COBICISTAT 150 MG                                         |Tableta      |2024      |Noviembre 25|13.0               |13              |\n",
            "|010.000.6318.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DOLUTEGRAVIR 10 MG                                                          |Tableta      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.6010.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DOLUTEGRAVIR 50 MG                                                          |Tableta      |2024      |Noviembre 25|1.0                |1               |\n",
            "|010.000.6108.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DOLUTEGRAVIR 50 MG, ABACAVIR 600 MG, LAMIVUDINA 300 MG                      |Tableta      |2024      |Noviembre 25|4.0                |4               |\n",
            "|010.000.7026.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DOLUTEGRAVIR 50 MG, LAMIVUDINA 300 MG                                       |TABLETA      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.7106.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DOLUTEGRAVIR 5MG TABLETA DISPERSABLE                                        |TABLETA      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.6320.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DORAVIRINA 100 MG                                                           |TABLETA      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.4370.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |EFAVIRENZ 600 MG                                                            |Tableta      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.5640.01  |ZACATECAS            |CAPASITS - Fresnillo|NULL |EFAVIRENZ 600 MG, EMTRICITABINA 200 MG, TENOFOVIR SUCCINATO 245 MG - TABLETA|Tableta      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.6162.00  |ZACATECAS            |CAPASITS - Guadalupe|NULL |EMTRICITABINA 200 MG, TENOFOVIR ALAFENAMIDA 10 MG                           |Tableta      |2024      |Noviembre 25|1.0                |1               |\n",
            "|010.000.6163.00  |ZACATECAS            |CAPASITS - Guadalupe|NULL |EMTRICITABINA 200 MG, TENOFOVIR ALAFENAMIDA 25 MG                           |TABLETAS     |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.4396.01  |ZACATECAS            |CAPASITS - Guadalupe|NULL |EMTRICITABINA 200 MG, TENOFOVIR SUCCINATO 245 MG - TABLETA                  |Tableta      |2024      |Noviembre 25|37.0               |37              |\n",
            "|010.000.4269.01  |ZACATECAS            |CAPASITS - Guadalupe|NULL |ENFUVIRTIDA 108 MG                                                          |Caja         |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.6074.00  |ZACATECAS            |CAPASITS - Guadalupe|NULL |ETRAVIRINA 200 MG                                                           |Tableta      |2024      |Noviembre 25|2.0                |2               |\n",
            "+-----------------+---------------------+--------------------+-----+----------------------------------------------------------------------------+-------------+----------+------------+-------------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------------+---------------------+--------------------+-----+--------------------------------------------------------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|clave_medicamento|establecimiento_salud|unidad_almacen      |corte|nombre_medicamento                                                  |unidad_medida|corte_anio|corte_mes   |consumo_arv_mensual|numero_pacientes|\n",
            "+-----------------+---------------------+--------------------+-----+--------------------------------------------------------------------+-------------+----------+------------+-------------------+----------------+\n",
            "|010.000.4272.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |ABACAVIR SOLUCION                                                   |ml           |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.6203.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |BICTEGRAVIR 50 MG, EMTRICITABINA 200 MG, TENOFOVIR ALAFENAMIDA 25 MG|Envase       |2024      |Noviembre 25|130.0              |130             |\n",
            "|010.000.5862.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 150 MG                                                    |COMPRIMIDOS  |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.5860.01  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 400 MG                                                    |Tableta      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.4289.01  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 600 MG                                                    |Tableta      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.5861.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 75 MG                                                     |COMPRIMIDOS  |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.6098.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DARUNAVIR 800 MG, COBICISTAT 150 MG                                 |Tableta      |2024      |Noviembre 25|13.0               |13              |\n",
            "|010.000.6318.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DOLUTEGRAVIR 10 MG                                                  |Tableta      |2024      |Noviembre 25|0.0                |0               |\n",
            "|010.000.6010.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DOLUTEGRAVIR 50 MG                                                  |Tableta      |2024      |Noviembre 25|1.0                |1               |\n",
            "|010.000.6108.00  |ZACATECAS            |CAPASITS - Fresnillo|NULL |DOLUTEGRAVIR 50 MG, ABACAVIR 600 MG, LAMIVUDINA 300 MG              |Tableta      |2024      |Noviembre 25|4.0                |4               |\n",
            "+-----------------+---------------------+--------------------+-----+--------------------------------------------------------------------+-------------+----------+------------+-------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfi.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eloNm-uThOKz",
        "outputId": "b8f892f7-674a-4b04-bf7c-7cc26e6c75a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------------+-------------+\n",
            "|consumo_arv_mensual|numero_pacientes|     features|\n",
            "+-------------------+----------------+-------------+\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|              130.0|             130|[130.0,130.0]|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|               13.0|              13|  [13.0,13.0]|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                1.0|               1|    [1.0,1.0]|\n",
            "|                4.0|               4|    [4.0,4.0]|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                1.0|               1|    [1.0,1.0]|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|               37.0|              37|  [37.0,37.0]|\n",
            "|                0.0|               0|    (2,[],[])|\n",
            "|                2.0|               2|    [2.0,2.0]|\n",
            "+-------------------+----------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=127\n",
        "b=140\n",
        "c=a+b\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk2m8JJxg5MF",
        "outputId": "2cd8e1c7-6352-48ec-856f-bd5922c81317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "267\n"
          ]
        }
      ]
    }
  ]
}